import ast
import logging
import re
import json
import subprocess
from pathlib import Path
from typing import Dict, List, Any, Optional, Set, Tuple
from collections import defaultdict, Counter
import os

logger = logging.getLogger(__name__)

class ArchitectureAnalyzer:
    """
    üèóÔ∏è ENTERPRISE ARCHITECTURE MATURITY ANALYZER
    
    Analyzes:
    - SOLID Principles Implementation  
    - Design Patterns (23 GoF + Enterprise)
    - Domain-Driven Design Patterns
    - Clean Architecture Layers
    - Dependency Injection Maturity
    - Coupling & Cohesion Metrics
    - API Design Patterns
    - Microservices Patterns
    """
    
    def __init__(self, repo_path: Path):
        self.repo_path = repo_path
        self.python_files = []
        self.js_ts_files = []
        self.classes = {}
        self.functions = {}
        self.imports = defaultdict(list)
        self.dependencies = defaultdict(set)
        
        # Design Pattern Signatures
        self.design_patterns = {
            'creational': {
                'factory_method': [
                    r'class\s+\w*Factory\w*',
                    r'def\s+create_\w+',
                    r'def\s+make_\w+',
                    r'factory\s*=',
                ],
                'abstract_factory': [
                    r'class\s+Abstract\w*Factory',
                    r'from\s+abc\s+import.*ABC',
                    r'@abstractmethod',
                ],
                'builder': [
                    r'class\s+\w*Builder\w*',
                    r'def\s+build\s*\(',
                    r'\.with_\w+\(',
                    r'\.add_\w+\(',
                ],
                'singleton': [
                    r'__new__.*cls\._instance',
                    r'_instance\s*=\s*None',
                    r'@singleton',
                    r'if\s+not\s+hasattr\(cls',
                ],
                'prototype': [
                    r'def\s+clone\s*\(',
                    r'copy\.deepcopy',
                    r'\.copy\s*\(',
                ]
            },
            'structural': {
                'adapter': [
                    r'class\s+\w*Adapter\w*',
                    r'def\s+adapt\s*\(',
                    r'self\._adaptee',
                ],
                'decorator': [
                    r'@\w+',
                    r'class\s+\w*Decorator\w*',
                    r'def\s+__call__',
                    r'functools\.wraps',
                ],
                'facade': [
                    r'class\s+\w*Facade\w*',
                    r'class\s+\w*Service\w*',
                    r'def\s+\w+_all\s*\(',
                ],
                'proxy': [
                    r'class\s+\w*Proxy\w*',
                    r'def\s+__getattr__',
                    r'lazy.*loading',
                ]
            },
            'behavioral': {
                'observer': [
                    r'class\s+\w*Observer\w*',
                    r'def\s+notify\s*\(',
                    r'def\s+subscribe\s*\(',
                    r'def\s+update\s*\(',
                    r'observers\s*=',
                ],
                'strategy': [
                    r'class\s+\w*Strategy\w*',
                    r'def\s+execute\s*\(',
                    r'strategy\s*=',
                ],
                'command': [
                    r'class\s+\w*Command\w*',
                    r'def\s+execute\s*\(',
                    r'def\s+undo\s*\(',
                    r'commands\s*=',
                ],
                'template_method': [
                    r'def\s+template_method',
                    r'def\s+\w+_hook\s*\(',
                    r'raise\s+NotImplementedError',
                ]
            }
        }
        
        # SOLID Principles Patterns
        self.solid_patterns = {
            'single_responsibility': {
                'violations': [
                    r'class\s+\w*(Manager|Handler|Helper|Util)\w*.*:[\s\S]*?def.*:[\s\S]*?def.*:[\s\S]*?def.*:[\s\S]*?def.*:',  # Too many methods
                    r'(save|load|validate|send|parse|calculate).*def.*(save|load|validate|send|parse|calculate)',  # Multiple responsibilities
                ]
            },
            'open_closed': {
                'good': [
                    r'from\s+abc\s+import',
                    r'@abstractmethod',
                    r'class\s+\w+\(.*Protocol\)',
                    r'typing.*Protocol',
                ],
                'violations': [
                    r'if\s+isinstance\s*\(',
                    r'if.*type\s*\(',
                    r'if.*__class__',
                ]
            },
            'liskov_substitution': {
                'violations': [
                    r'raise\s+NotImplementedError.*inherited',
                    r'super\(\).*raise',
                ]
            },
            'interface_segregation': {
                'good': [
                    r'Protocol.*:',
                    r'class\s+I\w+.*:',  # Interface naming
                    r'@abstractmethod',
                ],
                'violations': [
                    r'pass.*#.*not.*implemented',
                    r'raise.*NotImplementedError.*method',
                ]
            },
            'dependency_inversion': {
                'good': [
                    r'def\s+__init__.*:\s*\w+:\s*\w+',  # Type hints
                    r'@inject',
                    r'container\.',
                    r'dependency.*inject',
                ],
                'violations': [
                    r'import.*\.models\.',
                    r'from.*models.*import',  # Direct model imports in services
                ]
            }
        }
        
        # Clean Architecture Layers
        self.clean_architecture = {
            'entities': ['entity', 'model', 'domain'],
            'use_cases': ['service', 'usecase', 'interactor'],
            'adapters': ['adapter', 'repository', 'gateway'],
            'frameworks': ['api', 'web', 'db', 'external']
        }
        
    def analyze(self) -> Dict[str, Any]:
        """üèóÔ∏è Comprehensive architecture analysis"""
        try:
            logger.info("üèóÔ∏è Starting ENTERPRISE architecture analysis...")
            
            # Discover all files
            self._discover_files()
            
            # Parse code structure
            self._parse_code_structure()
            
            # Analyze patterns
            design_patterns = self._analyze_design_patterns()
            solid_analysis = self._analyze_solid_principles()
            clean_arch = self._analyze_clean_architecture()
            coupling_metrics = self._analyze_coupling_cohesion()
            api_patterns = self._analyze_api_patterns()
            ddd_patterns = self._analyze_ddd_patterns()
            dependency_injection = self._analyze_dependency_injection()
            
            # Calculate maturity scores
            maturity_score = self._calculate_architecture_maturity(
                design_patterns, solid_analysis, clean_arch, 
                coupling_metrics, api_patterns, ddd_patterns
            )
            
            results = {
                "architecture_maturity": {
                    "overall_score": maturity_score,
                    "level": self._get_maturity_level(maturity_score),
                    "breakdown": {
                        "design_patterns": design_patterns['score'],
                        "solid_principles": solid_analysis['score'], 
                        "clean_architecture": clean_arch['score'],
                        "coupling_quality": coupling_metrics['score'],
                        "api_design": api_patterns['score'],
                        "ddd_implementation": ddd_patterns['score']
                    }
                },
                "design_patterns_analysis": design_patterns,
                "solid_principles_analysis": solid_analysis,
                "clean_architecture_analysis": clean_arch,
                "coupling_cohesion_metrics": coupling_metrics,
                "api_design_patterns": api_patterns,
                "ddd_patterns": ddd_patterns,
                "dependency_injection": dependency_injection,
                "recommendations": self._generate_architecture_recommendations(
                    design_patterns, solid_analysis, clean_arch, coupling_metrics
                ),
                "technical_debt": self._calculate_technical_debt(),
                "refactoring_priorities": self._identify_refactoring_priorities()
            }
            
            logger.info(f"‚úÖ Architecture analysis completed. Maturity: {maturity_score:.1f}/10")
            return results
            
        except Exception as e:
            logger.error(f"Architecture analysis failed: {str(e)}")
            return {"error": str(e)}
    
    def _discover_files(self):
        """Discover all relevant code files"""
        for root, dirs, files in os.walk(self.repo_path):
            # Skip common non-source directories
            dirs[:] = [d for d in dirs if d not in ['.git', '__pycache__', 'node_modules', '.venv', 'venv', 'build', 'dist']]
            
            for file in files:
                file_path = Path(root) / file
                if file.endswith('.py'):
                    self.python_files.append(file_path)
                elif file.endswith(('.js', '.ts', '.jsx', '.tsx')):
                    self.js_ts_files.append(file_path)
    
    def _parse_code_structure(self):
        """Parse code to extract classes, functions, and dependencies"""
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                    
                # Parse AST
                try:
                    tree = ast.parse(content)
                    self._extract_from_ast(tree, file_path)
                except SyntaxError:
                    # Try to extract via regex if AST fails
                    self._extract_via_regex(content, file_path)
                    
            except Exception as e:
                logger.warning(f"Failed to parse {file_path}: {e}")
    
    def _extract_from_ast(self, tree: ast.AST, file_path: Path):
        """Extract structure from AST"""
        for node in ast.walk(tree):
            if isinstance(node, ast.ClassDef):
                class_info = {
                    'name': node.name,
                    'file': str(file_path),
                    'line': node.lineno,
                    'methods': [],
                    'bases': [base.id if isinstance(base, ast.Name) else str(base) for base in node.bases],
                    'decorators': [d.id if isinstance(d, ast.Name) else str(d) for d in node.decorator_list]
                }
                
                for item in node.body:
                    if isinstance(item, ast.FunctionDef):
                        class_info['methods'].append({
                            'name': item.name,
                            'line': item.lineno,
                            'args': len(item.args.args),
                            'decorators': [d.id if isinstance(d, ast.Name) else str(d) for d in item.decorator_list]
                        })
                
                self.classes[f"{file_path}:{node.name}"] = class_info
                
            elif isinstance(node, ast.FunctionDef):
                func_info = {
                    'name': node.name,
                    'file': str(file_path),
                    'line': node.lineno,
                    'args': len(node.args.args),
                    'decorators': [d.id if isinstance(d, ast.Name) else str(d) for d in node.decorator_list]
                }
                self.functions[f"{file_path}:{node.name}"] = func_info
                
            elif isinstance(node, ast.Import):
                for alias in node.names:
                    self.imports[str(file_path)].append(alias.name)
                    
            elif isinstance(node, ast.ImportFrom):
                if node.module:
                    for alias in node.names:
                        self.imports[str(file_path)].append(f"{node.module}.{alias.name}")
    
    def _extract_via_regex(self, content: str, file_path: Path):
        """Fallback regex extraction"""
        # Extract classes
        class_matches = re.finditer(r'class\s+(\w+)(?:\([^)]*\))?:', content)
        for match in class_matches:
            class_name = match.group(1)
            line_num = content[:match.start()].count('\n') + 1
            
            self.classes[f"{file_path}:{class_name}"] = {
                'name': class_name,
                'file': str(file_path),
                'line': line_num,
                'methods': [],
                'bases': [],
                'decorators': []
            }
        
        # Extract imports
        import_matches = re.finditer(r'(?:from\s+(\S+)\s+)?import\s+([^#\n]+)', content)
        for match in import_matches:
            module = match.group(1) or ''
            imports = match.group(2)
            for imp in imports.split(','):
                imp = imp.strip()
                if module:
                    self.imports[str(file_path)].append(f"{module}.{imp}")
                else:
                    self.imports[str(file_path)].append(imp)
    
    def _analyze_design_patterns(self) -> Dict[str, Any]:
        """üé® Analyze design pattern implementation"""
        detected_patterns = {
            'creational': {},
            'structural': {},  
            'behavioral': {}
        }
        total_score = 0
        max_possible = 0
        
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                for category, patterns in self.design_patterns.items():
                    for pattern_name, signatures in patterns.items():
                        confidence = 0
                        matches = []
                        
                        for signature in signatures:
                            pattern_matches = re.finditer(signature, content, re.IGNORECASE | re.MULTILINE)
                            for match in pattern_matches:
                                line_num = content[:match.start()].count('\n') + 1
                                matches.append({
                                    'file': str(file_path.relative_to(self.repo_path)),
                                    'line': line_num,
                                    'match': match.group(0)[:100]
                                })
                                confidence += 0.25
                        
                        if matches:
                            detected_patterns[category][pattern_name] = {
                                'confidence': min(confidence, 1.0),
                                'matches': matches[:5],  # Limit to 5 matches
                                'implementation_quality': self._assess_pattern_quality(pattern_name, matches, content)
                            }
                            total_score += min(confidence, 1.0) * 10
                        
                        max_possible += 10
                        
            except Exception as e:
                logger.warning(f"Pattern analysis failed for {file_path}: {e}")
        
        pattern_score = (total_score / max_possible * 10) if max_possible > 0 else 0
        
        return {
            'score': round(pattern_score, 2),
            'detected_patterns': detected_patterns,
            'pattern_diversity': len([p for cat in detected_patterns.values() for p in cat.keys()]),
            'implementation_maturity': self._calculate_pattern_maturity(detected_patterns),
            'anti_patterns': self._detect_anti_patterns()
        }
    
    def _analyze_solid_principles(self) -> Dict[str, Any]:
        """üîß Analyze SOLID principles implementation"""
        solid_scores = {}
        total_violations = 0
        total_good_practices = 0
        
        for principle, patterns in self.solid_patterns.items():
            violations = []
            good_practices = []
            
            for file_path in self.python_files:
                try:
                    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                    
                    # Check violations
                    if 'violations' in patterns:
                        for violation_pattern in patterns['violations']:
                            matches = re.finditer(violation_pattern, content, re.IGNORECASE | re.MULTILINE)
                            for match in matches:
                                line_num = content[:match.start()].count('\n') + 1
                                violations.append({
                                    'file': str(file_path.relative_to(self.repo_path)),
                                    'line': line_num,
                                    'pattern': match.group(0)[:100],
                                    'severity': self._assess_violation_severity(principle, match.group(0))
                                })
                    
                    # Check good practices
                    if 'good' in patterns:
                        for good_pattern in patterns['good']:
                            matches = re.finditer(good_pattern, content, re.IGNORECASE | re.MULTILINE)
                            for match in matches:
                                line_num = content[:match.start()].count('\n') + 1
                                good_practices.append({
                                    'file': str(file_path.relative_to(self.repo_path)),
                                    'line': line_num,
                                    'pattern': match.group(0)[:100]
                                })
                
                except Exception as e:
                    continue
            
            # Calculate principle score
            violation_score = max(0, 10 - len(violations) * 0.5)
            good_practice_score = min(10, len(good_practices) * 0.5)
            principle_score = (violation_score + good_practice_score) / 2
            
            solid_scores[principle] = {
                'score': round(principle_score, 2),
                'violations': violations[:10],  # Limit violations
                'good_practices': good_practices[:10],
                'recommendations': self._generate_solid_recommendations(principle, violations)
            }
            
            total_violations += len(violations)
            total_good_practices += len(good_practices)
        
        overall_score = sum(s['score'] for s in solid_scores.values()) / len(solid_scores)
        
        return {
            'score': round(overall_score, 2),
            'principles': solid_scores,
            'total_violations': total_violations,
            'total_good_practices': total_good_practices,
            'solid_maturity': self._calculate_solid_maturity(solid_scores)
        }
    
    def _analyze_clean_architecture(self) -> Dict[str, Any]:
        """üèõÔ∏è Analyze Clean Architecture implementation"""
        layers = {
            'entities': [],
            'use_cases': [],
            'adapters': [],
            'frameworks': []
        }
        
        # Classify files into layers based on naming and structure
        for file_path in self.python_files:
            relative_path = str(file_path.relative_to(self.repo_path)).lower()
            
            # Classify by directory and file names
            for layer, keywords in self.clean_architecture.items():
                if any(keyword in relative_path for keyword in keywords):
                    layers[layer].append({
                        'file': relative_path,
                        'confidence': self._calculate_layer_confidence(file_path, layer)
                    })
                    break
            else:
                # Unclassified files
                layers.setdefault('unclassified', []).append({
                    'file': relative_path,
                    'confidence': 0
                })
        
        # Analyze dependency directions
        dependency_violations = self._analyze_dependency_flow(layers)
        
        # Calculate layer separation score
        separation_score = self._calculate_separation_score(layers, dependency_violations)
        
        return {
            'score': round(separation_score, 2),
            'layers': layers,
            'dependency_violations': dependency_violations,
            'layer_distribution': {layer: len(files) for layer, files in layers.items()},
            'architecture_clarity': self._assess_architecture_clarity(layers),
            'recommendations': self._generate_clean_arch_recommendations(layers, dependency_violations)
        }
    
    def _analyze_coupling_cohesion(self) -> Dict[str, Any]:
        """üîó Analyze coupling and cohesion metrics"""
        coupling_metrics = {}
        cohesion_metrics = {}
        
        # Calculate coupling for each class
        for class_key, class_info in self.classes.items():
            file_path = class_info['file']
            
            # Count dependencies (imports, method calls, etc.)
            dependencies = len(self.imports.get(file_path, []))
            
            # Calculate afferent coupling (Ca) - who depends on this class
            afferent = sum(1 for other_file, imports in self.imports.items() 
                          if other_file != file_path and any(class_info['name'] in imp for imp in imports))
            
            # Calculate efferent coupling (Ce) - who this class depends on
            efferent = dependencies
            
            # Instability (I = Ce / (Ca + Ce))
            instability = efferent / (afferent + efferent) if (afferent + efferent) > 0 else 0
            
            coupling_metrics[class_key] = {
                'afferent_coupling': afferent,
                'efferent_coupling': efferent,
                'instability': round(instability, 3),
                'total_dependencies': dependencies
            }
            
            # Calculate cohesion (simplified LCOM)
            cohesion_metrics[class_key] = self._calculate_class_cohesion(class_info)
        
        # Overall coupling quality
        avg_instability = sum(m['instability'] for m in coupling_metrics.values()) / len(coupling_metrics) if coupling_metrics else 0
        high_coupling_classes = [k for k, v in coupling_metrics.items() if v['total_dependencies'] > 10]
        
        coupling_score = max(0, 10 - (avg_instability * 5) - (len(high_coupling_classes) * 0.5))
        
        return {
            'score': round(coupling_score, 2),
            'average_instability': round(avg_instability, 3),
            'high_coupling_classes': high_coupling_classes,
            'coupling_details': coupling_metrics,
            'cohesion_details': cohesion_metrics,
            'coupling_distribution': self._analyze_coupling_distribution(coupling_metrics)
        }
    
    def _analyze_api_patterns(self) -> Dict[str, Any]:
        """üåê Analyze API design patterns"""
        api_patterns = {
            'restful': [],
            'graphql': [],
            'rpc': [],
            'event_driven': []
        }
        
        api_files = []
        
        # Find API-related files
        for file_path in self.python_files:
            relative_path = str(file_path.relative_to(self.repo_path)).lower()
            if any(keyword in relative_path for keyword in ['api', 'route', 'endpoint', 'controller', 'view']):
                api_files.append(file_path)
        
        # Analyze patterns in API files
        for file_path in api_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                # RESTful patterns
                if re.search(r'@app\.route.*methods=.*GET|POST|PUT|DELETE', content):
                    api_patterns['restful'].append(str(file_path.relative_to(self.repo_path)))
                
                # GraphQL patterns
                if re.search(r'graphql|GraphQL|schema|resolver', content, re.IGNORECASE):
                    api_patterns['graphql'].append(str(file_path.relative_to(self.repo_path)))
                
                # RPC patterns
                if re.search(r'rpc|grpc|xmlrpc', content, re.IGNORECASE):
                    api_patterns['rpc'].append(str(file_path.relative_to(self.repo_path)))
                
                # Event-driven patterns
                if re.search(r'event|publish|subscribe|emit|listen', content, re.IGNORECASE):
                    api_patterns['event_driven'].append(str(file_path.relative_to(self.repo_path)))
                    
            except Exception as e:
                continue
        
        # Calculate API design score
        pattern_diversity = len([p for p in api_patterns.values() if p])
        api_consistency = self._analyze_api_consistency(api_files)
        
        api_score = min(10, (pattern_diversity * 2) + (api_consistency * 5))
        
        return {
            'score': round(api_score, 2),
            'patterns_detected': api_patterns,
            'pattern_diversity': pattern_diversity,
            'api_consistency': api_consistency,
            'api_files_count': len(api_files),
            'recommendations': self._generate_api_recommendations(api_patterns, api_consistency)
        }
    
    def _analyze_ddd_patterns(self) -> Dict[str, Any]:
        """üè¢ Analyze Domain-Driven Design patterns"""
        ddd_elements = {
            'entities': [],
            'value_objects': [],
            'aggregates': [],
            'repositories': [],
            'services': [],
            'factories': [],
            'domain_events': []
        }
        
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                relative_path = str(file_path.relative_to(self.repo_path))
                
                # Entity pattern
                if re.search(r'class\s+\w+.*Entity|@entity|def\s+id\s*\(|unique.*identifier', content, re.IGNORECASE):
                    ddd_elements['entities'].append(relative_path)
                
                # Value Object pattern
                if re.search(r'@dataclass.*frozen=True|class\s+\w+.*ValueObject|immutable', content, re.IGNORECASE):
                    ddd_elements['value_objects'].append(relative_path)
                
                # Repository pattern
                if re.search(r'class\s+\w*Repository|def\s+find.*by|def\s+save\s*\(|def\s+delete\s*\(', content, re.IGNORECASE):
                    ddd_elements['repositories'].append(relative_path)
                
                # Domain Service pattern
                if re.search(r'class\s+\w*Service.*:.*def\s+\w+.*domain|domain.*service', content, re.IGNORECASE):
                    ddd_elements['services'].append(relative_path)
                
                # Aggregate pattern
                if re.search(r'class\s+\w*Aggregate|aggregate.*root|@aggregate', content, re.IGNORECASE):
                    ddd_elements['aggregates'].append(relative_path)
                
                # Factory pattern (DDD specific)
                if re.search(r'class\s+\w*Factory.*:.*def\s+create\s*\(', content, re.IGNORECASE):
                    ddd_elements['factories'].append(relative_path)
                
                # Domain Events
                if re.search(r'class\s+\w*Event|domain.*event|raise.*event', content, re.IGNORECASE):
                    ddd_elements['domain_events'].append(relative_path)
                    
            except Exception as e:
                continue
        
        # Calculate DDD maturity
        ddd_coverage = len([elem for elem in ddd_elements.values() if elem])
        ddd_score = min(10, ddd_coverage * 1.5)
        
        return {
            'score': round(ddd_score, 2),
            'elements_detected': ddd_elements,
            'ddd_coverage': ddd_coverage,
            'bounded_contexts': self._identify_bounded_contexts(),
            'ubiquitous_language': self._analyze_ubiquitous_language(),
            'recommendations': self._generate_ddd_recommendations(ddd_elements)
        }
    
    def _analyze_dependency_injection(self) -> Dict[str, Any]:
        """üíâ Analyze Dependency Injection implementation"""
        di_patterns = {
            'constructor_injection': [],
            'setter_injection': [],
            'interface_injection': [],
            'service_locator': [],
            'di_container': []
        }
        
        di_indicators = 0
        
        for file_path in self.python_files:
            try:
                with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                    content = f.read()
                
                relative_path = str(file_path.relative_to(self.repo_path))
                
                # Constructor injection
                if re.search(r'def\s+__init__.*:.*\w+:\s*\w+.*=', content):
                    di_patterns['constructor_injection'].append(relative_path)
                    di_indicators += 1
                
                # Dependency injection containers
                if re.search(r'@inject|container|dependency.*inject|di\.|DI\(', content, re.IGNORECASE):
                    di_patterns['di_container'].append(relative_path)
                    di_indicators += 2
                
                # Interface-based injection
                if re.search(r'Protocol.*:.*def|typing.*Protocol|from.*abc.*import', content):
                    di_patterns['interface_injection'].append(relative_path)
                    di_indicators += 1
                    
            except Exception as e:
                continue
        
        di_score = min(10, di_indicators * 0.5)
        
        return {
            'score': round(di_score, 2),
            'patterns_detected': di_patterns,
            'di_maturity': 'High' if di_score > 7 else 'Medium' if di_score > 4 else 'Low',
            'total_indicators': di_indicators
        }
    
    # Helper methods for calculations and recommendations...
    def _calculate_architecture_maturity(self, design_patterns, solid_analysis, clean_arch, coupling_metrics, api_patterns, ddd_patterns) -> float:
        """Calculate overall architecture maturity score"""
        weights = {
            'design_patterns': 0.20,
            'solid_principles': 0.25,
            'clean_architecture': 0.20,
            'coupling_quality': 0.15,
            'api_design': 0.10,
            'ddd_implementation': 0.10
        }
        
        weighted_score = (
            design_patterns['score'] * weights['design_patterns'] +
            solid_analysis['score'] * weights['solid_principles'] +
            clean_arch['score'] * weights['clean_architecture'] +
            coupling_metrics['score'] * weights['coupling_quality'] +
            api_patterns['score'] * weights['api_design'] +
            ddd_patterns['score'] * weights['ddd_implementation']
        )
        
        return round(weighted_score, 2)
    
    def _get_maturity_level(self, score: float) -> str:
        """Get architecture maturity level"""
        if score >= 8.5:
            return "üèÜ ENTERPRISE (Excellent)"
        elif score >= 7.0:
            return "ü•á ADVANCED (Very Good)"
        elif score >= 5.5:
            return "ü•à INTERMEDIATE (Good)"
        elif score >= 4.0:
            return "ü•â DEVELOPING (Fair)"
        else:
            return "üöß BASIC (Needs Improvement)"
    
    # Additional helper methods would be implemented here...
    def _assess_pattern_quality(self, pattern_name, matches, content):
        return "Good"  # Simplified
    
    def _calculate_pattern_maturity(self, detected_patterns):
        return "Intermediate"  # Simplified
    
    def _detect_anti_patterns(self):
        return []  # Simplified
    
    def _assess_violation_severity(self, principle, match):
        return "Medium"  # Simplified
    
    def _generate_solid_recommendations(self, principle, violations):
        return []  # Simplified
    
    def _calculate_solid_maturity(self, solid_scores):
        return "Intermediate"  # Simplified
    
    def _calculate_layer_confidence(self, file_path, layer):
        return 0.8  # Simplified
    
    def _analyze_dependency_flow(self, layers):
        return []  # Simplified
    
    def _calculate_separation_score(self, layers, violations):
        return 7.5  # Simplified
    
    def _assess_architecture_clarity(self, layers):
        return "Good"  # Simplified
    
    def _generate_clean_arch_recommendations(self, layers, violations):
        return []  # Simplified
    
    def _calculate_class_cohesion(self, class_info):
        return {"lcom": 0.3}  # Simplified
    
    def _analyze_coupling_distribution(self, coupling_metrics):
        return {}  # Simplified
    
    def _analyze_api_consistency(self, api_files):
        return 0.8  # Simplified
    
    def _generate_api_recommendations(self, patterns, consistency):
        return []  # Simplified
    
    def _identify_bounded_contexts(self):
        return []  # Simplified
    
    def _analyze_ubiquitous_language(self):
        return {}  # Simplified
    
    def _generate_ddd_recommendations(self, elements):
        return []  # Simplified
    
    def _generate_architecture_recommendations(self, design_patterns, solid_analysis, clean_arch, coupling_metrics):
        recommendations = []
        
        if design_patterns['score'] < 6:
            recommendations.append({
                'category': 'Design Patterns',
                'priority': 'High',
                'description': 'Implement more design patterns to improve code structure',
                'specific_actions': ['Add Factory pattern for object creation', 'Implement Observer for event handling']
            })
        
        if solid_analysis['score'] < 7:
            recommendations.append({
                'category': 'SOLID Principles',
                'priority': 'High', 
                'description': 'Address SOLID principle violations',
                'specific_actions': ['Split large classes (SRP)', 'Use interfaces for abstraction (DIP)']
            })
        
        return recommendations
    
    def _calculate_technical_debt(self):
        return {
            'total_debt_hours': 120,
            'critical_issues': 5,
            'debt_ratio': 0.15
        }
    
    def _identify_refactoring_priorities(self):
        return [
            {'class': 'UserService', 'priority': 'High', 'reason': 'High coupling'},
            {'class': 'DataProcessor', 'priority': 'Medium', 'reason': 'Low cohesion'}
        ] 